{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annaGrd/Internship/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pFjBQbVIqZJ"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6GGpenZJ48VV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4192cf3-56f0-4709-d217-56d22d0bc8dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.4.0.post0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pytorch_lightning) (12.5.40)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch_lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch_lightning) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -qU\n",
        "!pip install einops\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNVdSouMSOt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf3f802-383b-4c2b-d8e9-8db26953ebc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import wandb\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import scipy.stats\n",
        "from skimage.util import random_noise\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "chemin = '/content/drive/My Drive/Stage/StageAnna/'\n",
        "if not os.path.isdir(chemin):\n",
        "    os.mkdir(chemin)\n",
        "os.chdir(chemin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbBGK3jNYyhM"
      },
      "source": [
        "# Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC-BC0yFIoNL"
      },
      "outputs": [],
      "source": [
        "from opts import parser\n",
        "from utils import dataloaders\n",
        "# args, unknown = parser.parse_known_args()\n",
        "\n",
        "def arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Classifying natural images with complex-valued neural networks\")\n",
        "\n",
        "    parser.add_argument('--filename', type=str, default=\"test\")\n",
        "    parser.add_argument('--lr', '--learning-rate', default=1e-3, type=float)\n",
        "    parser.add_argument('--batch-size', default=256, type=int)\n",
        "    parser.add_argument('--epochs', default=1, type=int)\n",
        "    parser.add_argument('--num_classes', default=10, type=int)\n",
        "    parser.add_argument('--noise_type', default=None)\n",
        "    parser.add_argument('--load', default=False, type=bool)\n",
        "    parser.add_argument('--save', default=False, type=bool)\n",
        "\n",
        "    parser.add_argument('--model', type=str, default='AlexNet_complex')\n",
        "\n",
        "    return parser.parse_args(\"\")\n",
        "\n",
        "args = arguments()\n",
        "\n",
        "chemin = f\"/content/drive/My Drive/Stage/StageAnna/Image/test_VGG16\"\n",
        "if not os.path.isdir(chemin):\n",
        "    os.mkdir(chemin)\n",
        "if not os.path.isdir(chemin+f\"/{args.model}\"):\n",
        "    os.mkdir(chemin+f\"/{args.model}\")\n",
        "    #os.mkdir(chemin+f\"/{args.model}/Spatial\")\n",
        "    #os.mkdir(chemin+f\"/{args.model}/Polar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br70ulPxIyZy"
      },
      "source": [
        "# Device configuration, Hyper-parameters and Classes Names\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYk8-pN_IzOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e2265b8-b6b1-46cd-a9a8-f61adb8575d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manna-grd\u001b[0m (\u001b[33mcomplex-dnns\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# login with wandb\n",
        "wandb.login()\n",
        "\n",
        "# Hyper-parameters\n",
        "# num_epochs = 4\n",
        "batch_size = args.batch_size\n",
        "learning_rate = args.lr\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "          'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDN21TBJI-Q4"
      },
      "source": [
        "# Choice of architecture and loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eYzkoJ1hJA65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a6ad85-636a-485d-d645-189c489498bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "if args.model == 'AlexNet_real_small':\n",
        "    from models.AlexNet_real_small import AlexNet\n",
        "    train_loader, val_loader = dataloaders.RGBtrain_data()\n",
        "    test_loader = dataloaders.RGBtest_data()\n",
        "elif args.model == 'AlexNet_complex_bio':\n",
        "    from models.AlexNet_complex_bio import AlexNet\n",
        "    train_loader, val_loader = dataloaders.iget_train_data()\n",
        "    test_loader = dataloaders.iget_test_data()\n",
        "elif args.model == 'AlexNet_complex':\n",
        "    loader_path = \"/content/drive/My Drive/Stage/StageAnna/Image/_trained_model\"\n",
        "    from models.AlexNet_complex import ComplexWeigth_AlexNet, AlexNet\n",
        "    train_loader, val_loader = dataloaders.iget_train_data()\n",
        "    test_loader = dataloaders.iget_test_data()\n",
        "elif args.model == 'VGG11_complex':\n",
        "    from models.VGG_complex import VGG11\n",
        "    train_loader, val_loader = dataloaders.iget_train_data()\n",
        "    test_loader = dataloaders.iget_test_data()\n",
        "elif args.model == 'VGG16_complex':\n",
        "    from models.VGG_complex import VGG16\n",
        "    train_loader, val_loader = dataloaders.iget_train_data()\n",
        "    test_loader = dataloaders.iget_test_data()\n",
        "elif args.model == 'VGG11_real':\n",
        "    from models.VGG_real import VGG11\n",
        "    train_loader, val_loader = dataloaders.RGBtrain_data()\n",
        "    test_loader = dataloaders.RGBtest_data()\n",
        "elif args.model == 'VGG16_real':\n",
        "    from models.VGG_real import VGG16\n",
        "    train_loader, val_loader = dataloaders.RGBtrain_data()\n",
        "    test_loader = dataloaders.RGBtest_data()\n",
        "elif args.model == 'VGG13_complex':\n",
        "    from models.VGG_complex import VGG13\n",
        "    train_loader, val_loader = dataloaders.iget_train_data()\n",
        "    test_loader = dataloaders.iget_test_data()\n",
        "elif args.model == 'VGG13_real':\n",
        "    from models.VGG_real import VGG13\n",
        "    train_loader, val_loader = dataloaders.RGBtrain_data()\n",
        "    test_loader = dataloaders.RGBtest_data()\n",
        "elif args.model == 'VGG19_complex':\n",
        "    from models.VGG_complex import VGG19\n",
        "    train_loader, val_loader = dataloaders.iget_train_data()\n",
        "    test_loader = dataloaders.iget_test_data()\n",
        "elif args.model == 'VGG19_real':\n",
        "    from models.VGG_real import VGG19\n",
        "    train_loader, val_loader = dataloaders.RGBtrain_data()\n",
        "    test_loader = dataloaders.RGBtest_data()\n",
        "else : # args.model == 'AlexNet_real':\n",
        "    from models.AlexNet_real import AlexNet\n",
        "    loader_path = \"/content/drive/My Drive/Stage/StageAnna/Image/_trained_model\"\n",
        "    train_loader, val_loader = dataloaders.RGBtrain_data()\n",
        "    test_loader = dataloaders.RGBtest_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8w9dJFoYPp0"
      },
      "source": [
        "# Plot d'image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ctK3NviPPH_"
      },
      "outputs": [],
      "source": [
        "def plot_mean_image(c1, mx1, c2, mx2, c3, c4, c5, mx5, idx):\n",
        "\n",
        "    layers = [(c1, \"Convolution 1\"), (mx1, \"MaxPool 1\"), (c2, \"Convolution 2\"),\n",
        "        (mx2, \"MaxPool 2\"),(c3, \"Convolution 3\"), (c4, \"Convolution 4\"),\n",
        "        (c5, \"Convolution 5\"), (mx5, \"MaxPool 5\")]\n",
        "\n",
        "    fig, axs = plt.subplots(2, 4, squeeze=False, layout='tight', figsize=[80, 40])\n",
        "    nb_layer = -1\n",
        "\n",
        "    for (layer, name) in layers:   # each operation on a single image\n",
        "\n",
        "        nb_layer += 1\n",
        "        ax = axs.flat[nb_layer]\n",
        "        edge = layer.shape[-1]\n",
        "\n",
        "        phase = layer[idx,:,:,:].angle()\n",
        "        phase = phase.detach().cpu().numpy()\n",
        "\n",
        "        mean = scipy.stats.circmean(phase, axis=0, high=np.pi, low=-np.pi)\n",
        "\n",
        "        magnitude = layer[idx,:,:,:].abs()\n",
        "        magnitude = magnitude.mean(0, True)\n",
        "        magnitude = magnitude.detach().cpu().numpy()\n",
        "        if magnitude.max():\n",
        "            magnitude = magnitude / magnitude.max()\n",
        "\n",
        "        levels = MaxNLocator(nbins=15).tick_values(phase.min(), phase.max())\n",
        "\n",
        "        y, x = np.mgrid[slice(edge), slice(edge)]\n",
        "\n",
        "        cmap = plt.colormaps['hsv']\n",
        "        ax.set(aspect='equal', adjustable='box')\n",
        "\n",
        "        im = ax.pcolormesh(x, y, mean, cmap=cmap, vmin=-np.pi, vmax=np.pi, alpha=magnitude) #, norm=norm\n",
        "        ax.title.set_text(name)\n",
        "\n",
        "    plt.savefig(chemin+f\"/{args.model}/Spatial/mean.png\")\n",
        "    plt.clf()\n",
        "    #plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy8lNCMMmAxa"
      },
      "outputs": [],
      "source": [
        "def plot_image(c1, mx1, c2, mx2, c3, c4, c5, mx5, idx):\n",
        "\n",
        "    layers = [(c1, \"Convolution 1\")]\n",
        "    # , (mx1, \"MaxPool 1\"), (c2, \"Convolution 2\"),\n",
        "        # (mx2, \"MaxPool 2\"),]\n",
        "    \"\"\"\n",
        "    (c3, \"Convolution 3\"), (c4, \"Convolution 4\"),\n",
        "    (c5, \"Convolution 5\"), (mx5, \"MaxPool 5\")]\n",
        "    \"\"\"\n",
        "\n",
        "    for (layer, name) in layers:   # each operation on a single image\n",
        "\n",
        "        features = len(layer[idx,:, 0,0])\n",
        "        row = 8\n",
        "        column = features // 8\n",
        "        fig, axs = plt.subplots(row, column, sharex=True, sharey=True, squeeze=False, layout='tight', figsize=[5*column,5*row])\n",
        "\n",
        "        for feature in range(features):\n",
        "            edge = len(layer[idx, feature,0,:])\n",
        "            phase = layer[idx,feature,:,:].angle()\n",
        "            magnitude = layer[idx,feature,:,:].abs()\n",
        "\n",
        "            phase = phase.detach().cpu().numpy()\n",
        "            magnitude = magnitude.detach().cpu().numpy()\n",
        "            if magnitude.max():\n",
        "                magnitude = magnitude / magnitude.max()\n",
        "            levels = MaxNLocator(nbins=15).tick_values(phase.min(), phase.max())\n",
        "\n",
        "            y, x = np.mgrid[slice(edge), slice(edge)]\n",
        "\n",
        "            cmap = plt.colormaps['hsv']\n",
        "            norm = mpl.colors.Normalize(-np.pi, np.pi)\n",
        "            #norm = BoundaryNorm(levels, ncolors=cmap.N, clip=True)\n",
        "            ax = axs.flat[feature]\n",
        "            ax.set(aspect='equal', adjustable='box')\n",
        "            im = ax.pcolormesh(x, y, phase, cmap=cmap, norm=norm, alpha=magnitude) #, norm=norm #vmin=-np.pi, vmax=np.pi\n",
        "\n",
        "\n",
        "        plt.title(f\"Operation : {name}\")\n",
        "        plt.savefig(chemin+f\"/{args.model}/{name} spatial.png\") #chemin+f\"/{args.model}/Spatial/{name}.png\"\n",
        "        plt.clf()\n",
        "        #plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFPEWlU_O6F9"
      },
      "source": [
        "# Plot de phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrl5C-055nSo"
      },
      "outputs": [],
      "source": [
        "def polar_mean_plot(c1, mx1, c2, mx2, c3, c4, c5, mx5, avg, c6, c7, c8, idx):\n",
        "\n",
        "    layers = [(c1, \"Convolution 1\"), (mx1, \"MaxPool 1\"), (c2, \"Convolution 2\"),\n",
        "        (mx2, \"MaxPool 2\"),(c3, \"Convolution 3\"), (c4, \"Convolution 4\"),\n",
        "        (c5, \"Convolution 5\"), (mx5, \"MaxPool 5\"), (avg, \"AvgPool\"),\n",
        "        (c6, \"Convolution 6\"), (c7, \"Convolution 7\") , (c8, \"Convolution 8\")]\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(3, 4, figsize=[80, 60], subplot_kw={'projection': 'polar'}, squeeze=False, layout='tight') #, sharex=True, sharey=True\n",
        "    nb_layer = -1\n",
        "\n",
        "    for (layer, name) in layers:   # each operation on a single image\n",
        "\n",
        "        nb_layer += 1\n",
        "        features = len(layer[idx,:, 0,0])\n",
        "        phase = layer[idx,:,:,:].angle()\n",
        "        phase = phase.detach().cpu().numpy()\n",
        "        phase = scipy.stats.circmean(phase, axis=0, high=np.pi, low=-np.pi)\n",
        "\n",
        "        magnitude = layer[idx,:,:,:].abs()\n",
        "        magnitude = magnitude.mean(0, True)\n",
        "        magnitude = magnitude.detach().cpu().numpy()\n",
        "\n",
        "        color = phase\n",
        "        cmap = plt.colormaps['hsv']\n",
        "        norm = mpl.colors.Normalize(-np.pi, np.pi)\n",
        "\n",
        "        ax = axs.flat[nb_layer]\n",
        "        ax.set_ylim([0, magnitude.max()])\n",
        "        ax.set(aspect='equal', adjustable='box')\n",
        "        ax.scatter(phase.flatten(), magnitude.flatten(), c=color.flatten(), cmap=cmap, norm=norm) #, c=phase.flatten\n",
        "        ax.title.set_text(name)\n",
        "\n",
        "    plt.savefig(chemin+f\"/{args.model}/Polar/mean.png\")\n",
        "    plt.clf()\n",
        "    #plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgrqNbGtmyBJ"
      },
      "outputs": [],
      "source": [
        "def polar_plot(c1, mx1, c2, mx2, c3, c4, c5, mx5, avg, c6, c7, c8, idx):\n",
        "\n",
        "    layers = [(c1, \"Convolution 1\")]\n",
        "    \"\"\"\n",
        "    , (mx1, \"MaxPool 1\"), (c2, \"Convolution 2\"),]\n",
        "    (mx2, \"MaxPool 2\"),(c3, \"Convolution 3\"),\n",
        "    (c4, \"Convolution 4\"),\n",
        "    (c5, \"Convolution 5\"), (mx5, \"MaxPool 5\"), (avg, \"AvgPool\"),\n",
        "    (c6, \"Convolution 6\"), (c7, \"Convolution 7\") , (c8, \"Convolution 8\")]\"\"\"\n",
        "\n",
        "    for (layer, name) in layers:   # each operation on a single image\n",
        "\n",
        "        features = len(layer[idx,:, 0,0])\n",
        "        row = 8\n",
        "        column = features // 8\n",
        "        fig, axs = plt.subplots(row, column, sharex=True, sharey=True, subplot_kw={'projection': 'polar'}, squeeze=False, layout='tight', figsize=[5*column,5*row])\n",
        "        for feature in range(features):\n",
        "\n",
        "            phase = layer[idx,feature,:,:].angle()\n",
        "            phase = phase.detach().cpu().numpy()\n",
        "            magnitude = layer[idx,feature,:,:].abs()\n",
        "            magnitude = magnitude.detach().cpu().numpy()\n",
        "\n",
        "            color = phase\n",
        "            cmap = plt.colormaps['hsv']\n",
        "            norm = mpl.colors.Normalize(-np.pi, np.pi)\n",
        "\n",
        "            ax = axs.flat[feature]\n",
        "            ax.set_ylim([0, magnitude.max()])\n",
        "            ax.set(aspect='equal', adjustable='box')\n",
        "            ax.scatter(phase.flatten(), magnitude.flatten(), c=color.flatten(), s=100, norm = norm, cmap='hsv') #, c=phase.flatten linewidths=0\n",
        "\n",
        "        plt.title(f\"Operation : {name}\")\n",
        "        plt.savefig(chemin+f\"/{args.model}/{name} polar.png\") #chemin+f\"/{args.model}/Polar/{name}.png\"\n",
        "        plt.clf()\n",
        "        #plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noise functions"
      ],
      "metadata": {
        "id": "yQi3wKpHnFT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_fog(images, intensity=0.5):\n",
        "    shape = False\n",
        "    if len(images.shape) == 3 :\n",
        "        images = rearrange(images, 'b c h -> h b c')\n",
        "        images = images[None,...]\n",
        "        shape = True\n",
        "    for i in range(images.shape[0]):\n",
        "        image = images[i]\n",
        "        #image = image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
        "        pil_image = Image.fromarray(image)\n",
        "\n",
        "        # Generate fog effect\n",
        "        w, h = image.shape[:2]\n",
        "        x, y = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n",
        "        fog = (x * y * intensity).astype(np.uint8)\n",
        "        fog_image = Image.fromarray(fog, mode='L')\n",
        "        merged = Image.merge(\"RGB\", (fog_image, fog_image, fog_image))\n",
        "\n",
        "        # Blend the original image with the fog effect\n",
        "        foggy_image = Image.blend(pil_image, merged, intensity)\n",
        "\n",
        "        # Convert back to PyTorch tensor\n",
        "        foggy_image = np.array(foggy_image).astype(np.float32)\n",
        "        foggy_tensor = torch.from_numpy(foggy_image) #.permute(2, 0, 1)\n",
        "\n",
        "        images[i] = foggy_tensor\n",
        "    if shape :\n",
        "        images.squeeze_(0)\n",
        "        images = rearrange(images, 'b c h -> c h b')\n",
        "\n",
        "    return images"
      ],
      "metadata": {
        "id": "UeU813IfnHgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_snow(image, snow_intensity=0.1):\n",
        "    w, h = image.size\n",
        "    snow = np.random.rand(h, w, 3) < snow_intensity\n",
        "    snow = (snow * 255).astype(np.uint8)\n",
        "    snow_image = Image.fromarray(snow, mode='RGB')\n",
        "    return Image.blend(image, snow_image, snow_intensity)"
      ],
      "metadata": {
        "id": "SnuP-4k-oVHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_motion_blur(image, kernel_size=15, angle=45):\n",
        "    # Créer un kernel de convolution pour le flou de mouvement\n",
        "    kernel = np.zeros((kernel_size, kernel_size))\n",
        "    kernel[int((kernel_size-1)/2), :] = np.ones(kernel_size)\n",
        "    kernel = cv2.warpAffine(kernel, cv2.getRotationMatrix2D((kernel_size/2-0.5, kernel_size/2-0.5), angle, 1.0), (kernel_size, kernel_size))\n",
        "    kernel = kernel / kernel_size\n",
        "\n",
        "    # Appliquer le kernel à l'image\n",
        "    blurred = cv2.filter2D(image, -1, kernel)\n",
        "    return blurred"
      ],
      "metadata": {
        "id": "GFmFZvpXorFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_glass_blur(image, kernel_size=3, iterations=5):\n",
        "    def get_random_shift(x, max_shift):\n",
        "        return x + np.random.randint(-max_shift, max_shift + 1)\n",
        "\n",
        "    blurred = image.copy()\n",
        "    for _ in range(iterations):\n",
        "        for i in range(image.shape[0]):\n",
        "            for j in range(image.shape[1]):\n",
        "                # Randomly select a point in the kernel\n",
        "                shift_x = get_random_shift(i, kernel_size // 2)\n",
        "                shift_y = get_random_shift(j, kernel_size // 2)\n",
        "                # Ensure the points are within the image boundaries\n",
        "                shift_x = np.clip(shift_x, 0, image.shape[0] - 1)\n",
        "                shift_y = np.clip(shift_y, 0, image.shape[1] - 1)\n",
        "                # Swap the pixel values\n",
        "                blurred[i, j] = image[shift_x, shift_y]\n",
        "    return blurred"
      ],
      "metadata": {
        "id": "w5zDReVHo94J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_zoom_blur(image, zoom_factor=1.2, iterations=10):\n",
        "    h, w, c = image.shape\n",
        "    center_x, center_y = w // 2, h // 2\n",
        "\n",
        "    # Create an empty image to accumulate the results\n",
        "    zoom_blur_image = np.zeros_like(image, dtype=np.float32)\n",
        "\n",
        "    # Apply zoom blur by scaling the image multiple times\n",
        "    for i in range(iterations):\n",
        "        scale = zoom_factor ** i\n",
        "        scaled_image = cv2.resize(image, (0, 0), fx=scale, fy=scale)\n",
        "\n",
        "        # Calculate the coordinates to center the scaled image\n",
        "        scaled_h, scaled_w, _ = scaled_image.shape\n",
        "        top = (scaled_h - h) // 2\n",
        "        left = (scaled_w - w) // 2\n",
        "        scaled_image = scaled_image[top:top+h, left:left+w]\n",
        "\n",
        "        # Blend the scaled image into the accumulated result\n",
        "        alpha = 1.0 / (i + 1)\n",
        "        zoom_blur_image = cv2.addWeighted(zoom_blur_image, 1.0 - alpha, scaled_image, alpha, 0)\n",
        "\n",
        "    # Clip the values to the valid range and convert back to uint8\n",
        "    zoom_blur_image = np.clip(zoom_blur_image, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return zoom_blur_image"
      ],
      "metadata": {
        "id": "Q40qZx0DpMf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_defocus_noise(image, sigma=1.0):\n",
        "    \"\"\"\n",
        "    Add defocused noise to a PyTorch tensor image.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): Input image tensor of shape (C, H, W).\n",
        "        sigma (float): Standard deviation of the Gaussian blur kernel.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Image tensor with added defocused noise.\n",
        "    \"\"\"\n",
        "    # Convert tensor to numpy array (H, W, C) and add noise\n",
        "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Create defocus kernel (Gaussian blur)\n",
        "    defocus_kernel_size = int(sigma * 4) | 1  # ensure it's odd\n",
        "    defocus_blur = cv2.GaussianBlur(image_np, (defocus_kernel_size, defocus_kernel_size), sigma)\n",
        "\n",
        "    # Add Gaussian noise\n",
        "    noise = np.random.normal(0, 1, image_np.shape)\n",
        "    defocus_noisy = defocus_blur + noise\n",
        "\n",
        "    # Clip values to [0, 255] and convert back to torch tensor\n",
        "    defocus_noisy = np.clip(defocus_noisy, 0, 255).astype(np.uint8)\n",
        "    defocus_noisy = torch.from_numpy(defocus_noisy).permute(2, 0, 1).float()\n",
        "\n",
        "    return defocus_noisy\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have an image tensor `input_image` of shape (C, H, W)\n",
        "# input_image = torch.randn(3, 256, 256)  # Example random input image\n",
        "# noisy_image = add_defocus_noise(input_image, sigma=1.0)\n",
        "\n",
        "# Note: Ensure you have OpenCV installed (`pip install opencv-python`) for GaussianBlur function.\n"
      ],
      "metadata": {
        "id": "m7V51cUqpkug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ic24zakehBo"
      },
      "source": [
        "#RGB image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVPeoyy8ejBI"
      },
      "outputs": [],
      "source": [
        "def rgb_plot(rgb_loader, index, noise_level, noise_type):\n",
        "    train_features, train_labels = next(iter(rgb_loader))\n",
        "    image = train_features[index]\n",
        "    image = rearrange(image, 'b c h -> c h b ')\n",
        "    if noise_type == 'gaussian':\n",
        "        image = torch.tensor(random_noise(image.cpu(), mode='gaussian', mean=noise_level))\n",
        "    elif noise_type == 's&p':\n",
        "        image = torch.tensor(random_noise(image.cpu(), mode='s&p', amount=noise_level))\n",
        "    elif noise_type == 'speckle':\n",
        "        print(noise_level)\n",
        "        image = torch.tensor(random_noise(image.cpu(), mode='speckle', mean=noise_level),\n",
        "                              dtype=torch.float).to(device)\n",
        "    elif noise_type == 'localvar':\n",
        "        noise_tensor = torch.ones(image.shape)*noise_level\n",
        "        image = torch.tensor(random_noise(image.cpu(), mode='localvar', local_vars=noise_tensor))\n",
        "    elif noise_type == 'fog':\n",
        "        image = torch.tensor(add_fog(image.cpu(), intensity=noise_level),\n",
        "                                dtype=torch.float).to(device)\n",
        "    elif noise_type == 'poisson':\n",
        "        image = torch.tensor(poisson_noise(image.cpu(), lmba=noise_level, clip=True))\n",
        "\n",
        "    label = train_labels[index]\n",
        "    plt.title(f\"Label: {classes[label]}, noise {noise_type}, level {noise_level}\")\n",
        "    plt.savefig(chemin+f\"/{args.model}/{index}_{noise_type}_{noise_level}.png\")\n",
        "    plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTQGTRg6JJVh"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab0bc17rJLAJ"
      },
      "outputs": [],
      "source": [
        "def training(model, num_epochs, epoch, train_loader, optimizer, criterion):\n",
        "    n_total_steps = len(train_loader)\n",
        "    model.train()\n",
        "\n",
        "    run_loss = 0.0\n",
        "    cnt = 0\n",
        "    total = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader): # to get all the different batches\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        #outputs = model(images)[-1]\n",
        "        outputs = model(images)\n",
        "        outputs_magnitude = outputs.abs()\n",
        "        loss = criterion(outputs_magnitude, labels)\n",
        "\n",
        "        run_loss += loss.item()\n",
        "        total += labels.size(0)\n",
        "        cnt += 1\n",
        "        _, predicted = torch.max(outputs_magnitude.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad() # empty the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch}')\n",
        "    return run_loss / cnt, correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCl6SaF2JRLB"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnnmqU0IJT2S"
      },
      "outputs": [],
      "source": [
        "def validation(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        n_correct = 0\n",
        "        n_samples = 0\n",
        "        n_class_correct = [ 0 for i in range(10)]\n",
        "        n_class_samples = [ 0 for i in range(10)]\n",
        "\n",
        "        correct, total, cnt = 0, 0, 0\n",
        "        run_loss = 0.0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            #outputs = model(images)[-1]\n",
        "            outputs = model(images)\n",
        "            outputs_magnitude = outputs.abs()\n",
        "            loss = criterion(outputs_magnitude, labels)\n",
        "\n",
        "            run_loss += loss\n",
        "            _, predicted = torch.max(outputs_magnitude.data, 1)\n",
        "            total += labels.size(0)\n",
        "            cnt += 1\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            for i in range(min(batch_size, len(labels))):\n",
        "                label = labels[i]\n",
        "                pred = predicted[i]\n",
        "                if (label == pred):\n",
        "                    n_class_correct[label] += 1\n",
        "                n_class_samples[label] += 1\n",
        "        acc = 100. * correct/total\n",
        "        print(f'Accuracy of the network: {acc} %')\n",
        "        #\n",
        "        # for i in range(10):\n",
        "        #     acc = 100. * n_class_correct[i] / n_class_samples[i]\n",
        "        #     print(f'Accuracy of {classes[i]} : {acc} %')\n",
        "\n",
        "    return run_loss / cnt, correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Poisson noise"
      ],
      "metadata": {
        "id": "-nO0zzjw74TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def poisson_noise(images, lmba=1, clip=False):\n",
        "\n",
        "    shape = False\n",
        "    if len(images.shape) == 3 :\n",
        "        images = rearrange(images, 'b c h -> h b c')\n",
        "        images = images[None,...]\n",
        "        shape = True\n",
        "\n",
        "    rng = np.random.default_rng(None)\n",
        "\n",
        "    for i in range(images.shape[0]):\n",
        "        image = images[i]\n",
        "\n",
        "        # Determine unique values in image & calculate the next power of two\n",
        "        vals = len(np.unique(image))\n",
        "        vals = 2 ** np.ceil(np.log2(vals))\n",
        "\n",
        "        # Detect if a signed image was input\n",
        "        if image.min() < 0:\n",
        "            low_clip = -1.0\n",
        "        else:\n",
        "            low_clip = 0.0\n",
        "\n",
        "        # Ensure image is exclusively positive\n",
        "        if low_clip == -1.0:\n",
        "            old_max = image.max()\n",
        "            image = (image + 1.0) / (old_max + 1.0)\n",
        "\n",
        "        # Generating noise for each unique value in image.\n",
        "        out = image * rng.poisson(lmba) / float(vals)\n",
        "\n",
        "        # Return image to original range if input was signed\n",
        "        if low_clip == -1.0:\n",
        "            out = out * (old_max + 1.0) - 1.0\n",
        "\n",
        "        if clip:\n",
        "            out = np.clip(out, low_clip, 1.0)\n",
        "\n",
        "        #images[i] = torch.from_numpy(out)\n",
        "        images[i] = out\n",
        "\n",
        "    if shape :\n",
        "        images.squeeze_(0)\n",
        "        images = rearrange(images, 'b c h -> c h b')\n",
        "\n",
        "    return images"
      ],
      "metadata": {
        "id": "z03Ry3R376Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-gYz5KgJXuB"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvpaTesdJZkB"
      },
      "outputs": [],
      "source": [
        "def testing(model, test_loader, criterion, noise_type, rgb_loader, noise_level):\n",
        "    # put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad(): # don't need the backward propagation\n",
        "        n_correct = 0\n",
        "        n_samples = 0\n",
        "        n_class_correct = [ 0 for i in range(10)]\n",
        "        n_class_samples = [ 0 for i in range(10)]\n",
        "\n",
        "        correct, total, cnt = 0, 0, 0\n",
        "        run_loss = 0.0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            if noise_type == 'gaussian':\n",
        "                images = torch.tensor(random_noise(images.cpu(), mode='gaussian', mean=noise_level, clip=True),\n",
        "                                      dtype=torch.float).to(device) #var=noiseLevel\n",
        "            elif noise_type == 's&p':\n",
        "                images = torch.tensor(random_noise(images.cpu(), mode='s&p', amount=noise_level, clip=True),\n",
        "                                      dtype=torch.float).to(device) #, amount=noiseLevel\n",
        "            elif noise_type == 'localvar':\n",
        "                noise_tensor = torch.ones(images.shape)*noise_level\n",
        "                images = torch.tensor(random_noise(images.cpu(), mode='localvar', local_vars=noise_tensor, clip=True),\n",
        "                                      dtype=torch.float).to(device)\n",
        "            elif noise_type == 'speckle':\n",
        "                images = torch.tensor(random_noise(images.cpu(), mode='speckle', mean=noise_level),\n",
        "                                      dtype=torch.float).to(device) #var=noiseLevel #clip=True\n",
        "            elif noise_type == 'fog':\n",
        "                images = torch.tensor(add_fog(images.cpu(), intensity=noise_level),\n",
        "                                      dtype=torch.float).to(device)\n",
        "            elif noise_type == 'poisson':\n",
        "                images = torch.tensor(poisson_noise(images.cpu(), lmba=noise_level, clip=True),\n",
        "                                      dtype=torch.float).to(device)\n",
        "            outputs = model(images)\n",
        "            #c1, mx1, c2, mx2, c3, c4, c5, mx5, avg, c6, c7, c8, outputs = model(images)\n",
        "\n",
        "            if not cnt:\n",
        "                idx = 0\n",
        "                # rgb_plot(rgb_loader, idx, noise_level, noise_type)\n",
        "                #plot_image(c1, mx1, c2, mx2, c3, c4, c5, mx5, idx)\n",
        "                #polar_plot(c1, mx1, c2, mx2, c3, c4, c5, mx5, avg, c6, c7, c8, idx)\n",
        "\n",
        "            outputs_magnitude = outputs.abs()\n",
        "            loss = criterion(outputs_magnitude, labels)\n",
        "\n",
        "            run_loss += loss\n",
        "            _, predicted = torch.max(outputs_magnitude.data, 1)\n",
        "            total += labels.size(0)\n",
        "            cnt += 1\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            for i in range(min(batch_size, len(labels))):\n",
        "                label = labels[i]\n",
        "                pred = predicted[i]\n",
        "                if (label == pred):\n",
        "                    n_class_correct[label] += 1\n",
        "                n_class_samples[label] += 1\n",
        "        acc = 100. * correct/total\n",
        "        #print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    return run_loss / cnt, correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chl-pXOxZMIN"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHLhoPMRIiFc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597,
          "referenced_widgets": [
            "e85e61a503ab4022b3753209e8390a65",
            "98754c3e734b432f8dd6e6c23fb92707",
            "b8ca89e060d447f59d1fd69e09ec49a9",
            "d07d45046fb14657a50cfda5aec1e4a7",
            "7db77fb5ec804daa963c5ce10714e659",
            "90c8468b6c5c4065b3558874f2fe1555",
            "90598d14db4944a5a363b7092f1250d9",
            "c51742630dcb4638bb91fafb2f7cceb3"
          ]
        },
        "outputId": "2534b182-7aef-4793-8476-b3facc351163",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/.shortcut-targets-by-id/1MA9zjGo2j7oX_jxA4NXywCxZtDSzMlr1/StageAnna/wandb/run-20240617_091444-t9eay1uy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/complex-dnns/internship-anna/runs/t9eay1uy' target=\"_blank\">VGG19_complex_test</a></strong> to <a href='https://wandb.ai/complex-dnns/internship-anna' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/complex-dnns/internship-anna' target=\"_blank\">https://wandb.ai/complex-dnns/internship-anna</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/complex-dnns/internship-anna/runs/t9eay1uy' target=\"_blank\">https://wandb.ai/complex-dnns/internship-anna/runs/t9eay1uy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "\n",
            "Epoch 0\n",
            "Accuracy of the network: 9.53 %\n",
            "Saving ... \n",
            "Train_loss: 2.451591078642827\n",
            "Val: 2.315023899078369\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.084 MB of 0.084 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e85e61a503ab4022b3753209e8390a65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "W&B sync reduced upload amount by 4.4%             "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>▁</td></tr><tr><td>Epoch</td><td>▁</td></tr><tr><td>Train Accuracy</td><td>▁</td></tr><tr><td>Train Loss</td><td>▁</td></tr><tr><td>Val Accuracy</td><td>▁</td></tr><tr><td>Val Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.0953</td></tr><tr><td>Epoch</td><td>1</td></tr><tr><td>Train Accuracy</td><td>0.0953</td></tr><tr><td>Train Loss</td><td>2.45159</td></tr><tr><td>Val Accuracy</td><td>0.0953</td></tr><tr><td>Val Loss</td><td>2.31502</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">VGG19_complex_test</strong> at: <a href='https://wandb.ai/complex-dnns/internship-anna/runs/t9eay1uy' target=\"_blank\">https://wandb.ai/complex-dnns/internship-anna/runs/t9eay1uy</a><br/> View project at: <a href='https://wandb.ai/complex-dnns/internship-anna' target=\"_blank\">https://wandb.ai/complex-dnns/internship-anna</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240617_091444-t9eay1uy/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def main(num_epochs, batch_size, learning_rate, classes, train_loader=train_loader, val_loader=val_loader, test_loader=test_loader, noise_type=None, load=False, save=False):\n",
        "\n",
        "\n",
        "    run = wandb.init(project='internship-anna', name=args.model+\"_test\", config={\n",
        "            \"epochs\": num_epochs,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"lr\": learning_rate,\n",
        "            \"model\": './results/'}, save_code=True)\n",
        "\n",
        "    config = run.config\n",
        "\n",
        "    train_loader = dataloaders.make_loader(train_loader, batch_size)\n",
        "    val_loader = dataloaders.make_loader(val_loader, batch_size)\n",
        "    RGBtrain_loader = dataloaders.RGBtest_data()\n",
        "    RGBtrain_loader = dataloaders.make_loader(RGBtrain_loader, batch_size)\n",
        "\n",
        "\n",
        "    #if args.model == 'AlexNet_real_small' or args.model == 'AlexNet_complex_bio':\n",
        "    if args.model == 'AlexNet_complex':\n",
        "        #ComplexWeigth_AlexNet, AlexNet\n",
        "        model = AlexNet(num_classes=args.num_classes).to(device)\n",
        "    elif args.model == 'VGG11_complex' or 'VGG11_real':\n",
        "        model = VGG11(num_classes=args.num_classes).to(device)\n",
        "    elif args.model == 'VGG13_complex' or 'VGG13_real':\n",
        "        model = VGG13(num_classes=args.num_classes).to(device)\n",
        "    elif args.model == 'VGG16_complex' or 'VGG16_real':\n",
        "        model = VGG16(num_classes=args.num_classes).to(device)\n",
        "    elif args.model == 'VGG19_complex' or 'VGG19_real':\n",
        "        model = VGG19(num_classes=args.num_classes).to(device)\n",
        "    else :\n",
        "        model = AlexNet(num_classes=args.num_classes).to(device)\n",
        "\n",
        "    if load :\n",
        "        model_path = loader_path+f\"/{args.model}.pth\"\n",
        "        dict_loaded = torch.load(model_path)\n",
        "        model.load_state_dict(dict_loaded['model'])\n",
        "        epochs = dict_loaded['epoch']\n",
        "        best_test_acc = dict_loaded['acc']\n",
        "    else :\n",
        "        epochs = 0\n",
        "        best_test_acc = 0.0\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    run.watch(model)\n",
        "    #print(sum([p.numel() for p in model.parameters() if p.requires_grad]))\n",
        "\n",
        "    # variables for storing losses during one epoch\n",
        "    train_loss, train_acc = [], []\n",
        "\n",
        "    for epoch in range(epochs, num_epochs+epochs):\n",
        "\n",
        "        train_loss, train_acc = training(model, num_epochs, epoch, train_loader, optimizer, criterion)\n",
        "        val_loss, val_acc = validation(model, val_loader, criterion)\n",
        "\n",
        "        if val_acc > best_test_acc:\n",
        "            print(f\"Saving ... \")\n",
        "            state = {\n",
        "                    'model': model.state_dict(),\n",
        "                    'acc': val_acc,\n",
        "                    'epoch': epoch,\n",
        "                    }\n",
        "\n",
        "            if save :\n",
        "                torch.save(state, model_path)\n",
        "            best_test_acc = val_acc\n",
        "\n",
        "\n",
        "        print(f\"Train_loss: {train_loss}\")\n",
        "        print(f\"Val: {val_loss}\")\n",
        "\n",
        "\n",
        "        wandb.log({\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Train Loss\": train_loss,\n",
        "            \"Train Accuracy\": train_acc,\n",
        "            \"Val Loss\": val_loss,\n",
        "            \"Val Accuracy\": val_acc,\n",
        "            \"Best Accuracy\": best_test_acc\n",
        "        })\n",
        "\n",
        "    test_loader = dataloaders.make_loader(test_loader, batch_size)\n",
        "    if noise_type == 'gaussian':\n",
        "        noiseLevel = [0, .15, .25, .35, .45, .6]\n",
        "    elif noise_type == 's&p':\n",
        "        noiseLevel = [0, .01, .03, .06, .1, .2]\n",
        "    elif noise_type == 'localvar':\n",
        "        noiseLevel = [.01, .03, .06, .1, .2]\n",
        "    elif noise_type == 'speckle':\n",
        "        noiseLevel = [0, 1, 5, 10, 15, 30]\n",
        "    elif noise_type == 'fog':\n",
        "        noiseLevel = [i/10 for i in range(8)]\n",
        "    elif noise_type == 'poisson':\n",
        "        noiseLevel = [20, 50, 100, 300, 600, 900, 1200]\n",
        "    else:\n",
        "        noiseLevel = []\n",
        "\n",
        "    for noise_level in noiseLevel:\n",
        "        test_loss, test_acc = testing(model, test_loader, criterion, noise_type, RGBtrain_loader, noise_level)\n",
        "        print(f\"{noise_type} noise, level {noise_level}\")\n",
        "        print(f\"Test Loss: {test_loss}\")\n",
        "        print(f\"Test Accuracy: {test_acc*100} %\")\n",
        "        print()\n",
        "\n",
        "        wandb.log({\n",
        "            \"Test Loss\": test_loss,\n",
        "            \"Test Accuracy\": test_acc\n",
        "        })\n",
        "\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "    return model\n",
        "\n",
        "model = main(args.epochs, args.batch_size, args.learning_rate, classes, train_loader=train_loader, val_loader=val_loader, test_loader=test_loader, noise_type=args.noise_type, load=args.load, save=args.save)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6pFjBQbVIqZJ",
        "LbBGK3jNYyhM",
        "br70ulPxIyZy",
        "yDN21TBJI-Q4",
        "W8w9dJFoYPp0",
        "SFPEWlU_O6F9",
        "yQi3wKpHnFT5",
        "XJryMH1XoTSJ",
        "slB48MDcooeR",
        "DZRp-jSXo7P4",
        "Db12eA1RpLHR",
        "TsRPMtBkpi_a",
        "_Ic24zakehBo",
        "JTQGTRg6JJVh",
        "PCl6SaF2JRLB",
        "0-gYz5KgJXuB"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNkhskXMahIAugB3Lhyuiiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e85e61a503ab4022b3753209e8390a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98754c3e734b432f8dd6e6c23fb92707",
              "IPY_MODEL_b8ca89e060d447f59d1fd69e09ec49a9"
            ],
            "layout": "IPY_MODEL_d07d45046fb14657a50cfda5aec1e4a7"
          }
        },
        "98754c3e734b432f8dd6e6c23fb92707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7db77fb5ec804daa963c5ce10714e659",
            "placeholder": "​",
            "style": "IPY_MODEL_90c8468b6c5c4065b3558874f2fe1555",
            "value": "0.211 MB of 0.211 MB uploaded (0.009 MB deduped)\r"
          }
        },
        "b8ca89e060d447f59d1fd69e09ec49a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90598d14db4944a5a363b7092f1250d9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c51742630dcb4638bb91fafb2f7cceb3",
            "value": 1
          }
        },
        "d07d45046fb14657a50cfda5aec1e4a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db77fb5ec804daa963c5ce10714e659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90c8468b6c5c4065b3558874f2fe1555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90598d14db4944a5a363b7092f1250d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c51742630dcb4638bb91fafb2f7cceb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}